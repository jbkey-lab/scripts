{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import sklearn\n",
    "import gc\n",
    "from numpy import asarray\n",
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "#BATCH_SIZE = BATCH_SIZE*strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "from numpy.random import seed \n",
    "seed(1)\n",
    "\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Tx = 30\n",
    "n_var = 9   # \"all\" : 9, \"weather\" : 7\n",
    "n_clusters = 20\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "data_dir = \"data_subset\"\n",
    "\n",
    "# Weather Variables, MG, Yield, Year, Location\n",
    "x_data = np.load(\"%s/mg_weather_variables_all_data_TS_%s.npy\" %(data_dir, Tx))   # (103365, 30, 8)\n",
    "y_data = np.load(\"%s/yield_year_location_all_data.npy\" %(data_dir))   # (103365, 3) yield, year, location\n",
    "\n",
    "# K-Means Clusters (Genotype)\n",
    "kmeans_clusters = np.load(\"%s/k_means_clusters_%s.npy\" %(data_dir, n_clusters))   # (5839, )\n",
    "\n",
    "# State\n",
    "state_data = read_csv(\"%s/state_list_all_data.csv\" %(data_dir), header=0, index_col=0)   # (103365, 1)\n",
    "state_data = state_data.values.tolist()\n",
    "\n",
    "# Genotype ID\n",
    "genotypeID = np.load(\"%s/genotype_id_all_data.npy\"%(data_dir))   # (103365, 1) max(genotype ID) = 5839\n",
    "\n",
    "# Genotype Cluster Sample Wise\n",
    "x_data_cluster = np.zeros((genotypeID.shape[0], 1))   # (103365, 1)\n",
    "for i in range(0, genotypeID.shape[0]):\n",
    "    id_genotype = genotypeID[i]\n",
    "    x_data_cluster[i] = kmeans_clusters [int(id_genotype - 1)]   # clusters [5838]\n",
    "    \n",
    "# Different Variables (Total- 8)\n",
    "# Column (0)- MG\n",
    "# Column (1)- Average Direct Normal Irradiance (ADNI)\n",
    "# Column (2)- Average Precipitation Previous Hour (inches) (AP)\n",
    "# Column (3)- Average Relative Humidity (ARH)\n",
    "# Column (4)- Maximum Direct Normal Irradiance (MDNI)\n",
    "# Column (5)- Maximum Surface Temperature (MaxSur)\n",
    "# Column (6)- Minimum Surface Temperature (MinSur)\n",
    "# Column (7)- Average Surface Temperature (Fahrenheit) (AvgSur)\n",
    "\n",
    "# MG\n",
    "x_data_mg = x_data [:, 0, 0]   # (103365, )\n",
    "x_data_mg = x_data_mg.reshape((x_data_mg.shape[0], 1))   # (103365, 1)\n",
    "\n",
    "# Append\n",
    "y_data = np.append(y_data, x_data_mg, axis=1)   # (103365, 4) yield, year, location, MG\n",
    "y_data = np.append(y_data, genotypeID, axis=1)   # (103365, 5) yield, year, location, MG, genotypeID\n",
    "y_data = np.append(y_data, x_data_cluster, axis=1)   # (103365, 6) yield, year, location, MG, genotypeID, Cluster\n",
    "\n",
    "# All Input Variables\n",
    "x_variables = np.zeros((x_data.shape[0], x_data.shape[1], n_var))   # (103365, 30, 9)\n",
    "\n",
    "# MG\n",
    "x_variables [:, :, 0] = x_data [:, :, 0]   \n",
    "\n",
    "# Cluster\n",
    "for i in range(0, x_variables.shape[0]):\n",
    "    x_variables[i, :, 1] = x_data_cluster[i]\n",
    "\n",
    "# Weather Variables\n",
    "x_variables[:, :, 2 :] = x_data [:, :, 1 :]   \n",
    "\n",
    "# Range of Indices\n",
    "indices = range (x_variables.shape[0])\n",
    "\n",
    "# Train, Validation and Test Sets\n",
    "x_train, x_val_test, y_train, y_val_test, in_train, in_val_test\\\n",
    "        = train_test_split(x_variables, y_data, indices, test_size = (val_size + test_size), random_state=42)\n",
    "\n",
    "# Validation, Test Sets\n",
    "x_val, x_test, y_val, y_test, in_val, in_test = train_test_split(x_val_test, y_val_test, in_val_test,\\\n",
    "                                                test_size=(test_size/(val_size+test_size)), random_state=42)       \n",
    "\n",
    "# State (Train, Validation, Test Sets)\n",
    "states_train = [state_data[index] for index in in_train]   # 82692 elements\n",
    "states_val = [state_data[index] for index in in_val]   # 10336 elements\n",
    "states_test = [state_data[index] for index in in_test]   # 10337 elements\n",
    "\n",
    "# Scale features \n",
    "# Two separate scalers for X, Y (diff dimensions) \n",
    "scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y =  MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "x_train_reshaped = x_train.reshape((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "\n",
    "yield_train = y_train [:, 0]   # (82692, )\n",
    "yield_train = yield_train.reshape((yield_train.shape[0], 1))   # (82692, 1)\n",
    "\n",
    "# Scaling Coefficients calculated from the training dataset\n",
    "scaler_x = scaler_x.fit(x_train_reshaped)   \n",
    "scaler_y = scaler_y.fit(yield_train)\n",
    "\n",
    "# Function to scale features after fitting\n",
    "def scale_features (data_x, data_y):\n",
    "    \n",
    "    data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2]))\n",
    "    data_x = scaler_x.transform(data_x)\n",
    "    data_x = data_x.reshape((data_x.shape[0], Tx, n_var))\n",
    "    \n",
    "    data_x_mg = data_x [:, 0, 0]\n",
    "    data_x_mg = data_x_mg.reshape((data_x_mg.shape[0], 1))   # (82692, 1)\n",
    "    data_x_cluster = data_x [:, 0, 1]\n",
    "    data_x_cluster = data_x_cluster.reshape((data_x_cluster.shape[0], 1))   # (82692, 1)\n",
    "    data_x_mg_cluster = np.append (data_x_mg, data_x_cluster, axis=1)   # (82692, 2)\n",
    "    \n",
    "    data_yield = data_y [:, 0]\n",
    "    data_yield = data_yield.reshape((data_yield.shape[0], 1))\n",
    "    data_yield = scaler_y.transform(data_yield)\n",
    "    \n",
    "    return data_x, data_x_mg_cluster, data_yield\n",
    "\n",
    "# Scale features\n",
    "x_train, x_train_mg_cluster, yield_train = scale_features (x_train, y_train)   # (82692,30,9), (82692,2), (82692, 1)\n",
    "x_val, x_val_mg_cluster, yield_val = scale_features (x_val, y_val)   # (10336, 30, 9), (10336, 2), (10336, 1)\n",
    "x_test, x_test_mg_cluster, yield_test = scale_features (x_test, y_test)   # (10337, 30, 9), (10337, 2), (10337, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
