{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-148d4aa4ff6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRectifiedAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "Dropout_model = 0.3856\n",
    "FVC_weight = 0.2\n",
    "Confidence_weight = 0\n",
    "#!pip install ../input/kerasapplications/keras-team-keras-applications-3b180cb -f ./ --no-index\n",
    "#!pip install ../input/efficientnet/efficientnet-1.1.0/ -f ./ --no-index\n",
    "import os\n",
    "#import cv2\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "from tqdm.notebook import tqdm \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow_addons.optimizers import RectifiedAdam\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "def seed_everything(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything(45)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "tr = pd.read_excel(open('../input/dryierbindatafornurserycorn/Drying formula.xlsx', 'rb'),\n",
    "              sheet_name='Sheet2') \n",
    "\n",
    "chunk =  tr.loc[tr.Datetype == \"Test\"] \n",
    "tr[\"ID\"] = range(1, 1 + len(tr))\n",
    "chunk[\"a\"] = \"sub\"\n",
    "chunk[\"b\"] = range(1, 1 + len(chunk))\n",
    "chunk[\"ID\"] = chunk[\"a\"] + chunk[\"b\"].map(str)\n",
    "\n",
    "del chunk[\"a\"]\n",
    "del chunk[\"b\"]\n",
    "print(chunk)\n",
    "\n",
    "#print(tr)\n",
    "#len(tr)\n",
    "#tr=tr.drop_duplicates(keep=False, inplace=True, subset=['ID','totalDays'])\n",
    "sub = pd.read_excel(open('../input/submission-dryier/Drying formula -sub.xlsx', 'rb'),\n",
    "              sheet_name='Submission') \n",
    "print(\"add infos\")\n",
    "sub['ID'] = sub['ID_day'].apply(lambda x:x.split('_')[0])\n",
    "sub['totalDays'] = sub['ID_day'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub =  sub[['ID','totalDays','Confidence','ID_day']]\n",
    "#del sub['ID']\n",
    "#sub[\"ID\"] = range(101, 101 + len(sub))\n",
    "\n",
    "tr = tr.dropna(how='any', subset=['moistureOffPercent','moistureOnPercent'])\n",
    "\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "#df.year.astype(int)\n",
    "#tr.drop_duplicates(keep=False, inplace=True, subset=['ID','totalDays'])\n",
    "\n",
    "sub = sub.merge(chunk.drop('totalDays', axis=1), on=\"ID\")\n",
    "tr['WHERE'] = 'train'\n",
    "chunk['WHERE'] = 'val'\n",
    "sub['WHERE'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr.head()\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tr.append([chunk,sub])\n",
    "print(tr.shape, chunk.shape, sub.shape, data.shape)\n",
    "print(tr.ID.nunique(), chunk.ID.nunique(), sub.ID.nunique(), \n",
    "      data.ID.nunique())\n",
    "\n",
    "data['min_day'] = data['totalDays']\n",
    "data.loc[data.WHERE=='test','min_day'] = np.nan\n",
    "data['min_day'] = data.groupby('ID')['min_day'].transform('min')\n",
    "base = data.loc[data.totalDays == data.min_day]\n",
    "base = base[['ID','moistureOffPercent']].copy()\n",
    "base.columns = ['ID','min_moistureOffPercent']\n",
    "base['nb'] = 1\n",
    "base['nb'] = base.groupby('ID')['nb'].transform('cumsum')\n",
    "base = base[base.nb==1]\n",
    "base.drop('nb', axis=1, inplace=True)\n",
    "data = data.merge(base, on='ID', how='left')\n",
    "data['base_week'] = data['totalDays'] - data['min_day']\n",
    "del base\n",
    "\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['Year'] #,'Age'\n",
    "FE = []\n",
    "for col in COLS:\n",
    "    for mod in data[col].unique():\n",
    "        FE.append(mod)\n",
    "        data[mod] = (data[col] == mod).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Moist'] = (data['moistureOnPercent'] - data['moistureOnPercent'].min() ) / ( data['moistureOnPercent'].max() - data['moistureOnPercent'].min() )\n",
    "#data['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) / ( data['min_FVC'].max() - data['min_FVC'].min() )\n",
    "#data['week'] = (data['base_week'] - data['base_week'].min() ) / ( data['base_week'].max() - data['base_week'].min() )\n",
    "#data['percent'] = (data['Percent'] - data['Percent'].min() ) / ( data['Percent'].max() - data['Percent'].min() )#\n",
    "#data['FVC_Percent'] = data['FVC'] / data['Percent']\n",
    "# FE += ['age','percent','week','BASE', 'FVC_Percent']\n",
    "FE += ['Moist','moistureOnPercent']\n",
    "tr = data.loc[data.WHERE=='train']\n",
    "chunk = data.loc[data.WHERE=='val']\n",
    "sub = data.loc[data.WHERE=='test']\n",
    "\n",
    "#del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in tr.columns: \n",
    "#    print(col) \n",
    "    \n",
    "tr=tr.drop(\"dateOn\",axis=1)\n",
    "tr=tr.drop(\"dateOff\",axis=1)\n",
    "#tr=tr.drop(\"ID_day\",axis=1)\n",
    "tr=tr.drop(\"moistureMidPercent\",axis=1)\n",
    "tr=tr.drop(\"FLD\",axis=1)\n",
    "tr=tr.drop(\"tempF\",axis=1)\n",
    "tr=tr.drop(\"Datetype\",axis=1)\n",
    "\n",
    "\n",
    "#for col in chunk.columns: \n",
    "#    print(col) \n",
    "    \n",
    "chunk=chunk.drop(\"dateOn\",axis=1)\n",
    "chunk=chunk.drop(\"dateOff\",axis=1)\n",
    "#chunk=chunk.drop(\"ID_day\",axis=1)\n",
    "chunk=chunk.drop(\"moistureMidPercent\",axis=1)\n",
    "chunk=chunk.drop(\"FLD\",axis=1)\n",
    "chunk=chunk.drop(\"tempF\",axis=1)\n",
    "chunk=chunk.drop(\"Datetype\",axis=1)\n",
    "\n",
    "#for col in sub.columns: \n",
    "#    print(col) \n",
    "    \n",
    "sub=sub.drop(\"dateOn\",axis=1)\n",
    "sub=sub.drop(\"dateOff\",axis=1)\n",
    "#sub=sub.drop(\"ID_day\",axis=1)\n",
    "sub=sub.drop(\"moistureMidPercent\",axis=1)\n",
    "sub=sub.drop(\"FLD\",axis=1)\n",
    "sub=sub.drop(\"tempF\",axis=1)\n",
    "sub=sub.drop(\"Datetype\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n",
    "def score(y_true, y_pred):\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "    \n",
    "    #sigma_clip = sigma + C1\n",
    "    sigma_clip = tf.maximum(sigma, C1)\n",
    "    delta = tf.abs(y_true[:, 0] - fvc_pred)\n",
    "    delta = tf.minimum(delta, C2)\n",
    "    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n",
    "    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n",
    "    return K.mean(metric)\n",
    "\n",
    "def qloss(y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    qs = [0.2, 0.50, 0.8]\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "\n",
    "def mloss(_lambda):\n",
    "    def loss(y_true, y_pred):\n",
    "        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def make_model(nh,D,ML):\n",
    "    #ML=ML\n",
    "    z = L.Input((nh,), name=\"Patient\")\n",
    "    x = L.Dense(D, activation=\"relu\", name=\"d1\")(z)\n",
    "    #x = L.Dense(D, activation=\"relu\", name=\"d2\")(x)\n",
    "    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n",
    "    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n",
    "    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n",
    "                     name=\"preds\")([p1, p2])\n",
    "    \n",
    "    model = M.Model(z, preds, name=\"CNN\")\n",
    "    model.compile(loss=mloss(ML), optimizer=\"sgd\", \n",
    "                  metrics=[\"poisson\"])\n",
    "    return model\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=100,monitor=\"val_loss\"),\n",
    "    #tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "     #                         patience=50, min_lr=0.001\n",
    "#),\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "print(FE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tr['moistureOffPercent'].values  # train target\n",
    "z = tr[FE].values  # fetures (1535, 9)\n",
    "print(z.shape)\n",
    "ze = sub[FE].values  # fetures of submission (730, 9) e: estimate\n",
    "print(ze.shape) \n",
    "nh = z.shape[1] \n",
    "print(nh)  # feature numbers (9,)\n",
    "\n",
    "#with strategy.scope():    \n",
    "net = make_model(nh,D=50,ML=.65)\n",
    "print(net.summary())\n",
    "print(net.count_params())\n",
    "NFOLD = 6 # originally 5\n",
    "kf = KFold(n_splits=NFOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(pe)\n",
    "del(pred)\n",
    "pe = np.zeros((ze.shape[0], 3))  #estimate of prediction\n",
    "pred = np.zeros((z.shape[0], 3))  # prediction of truth ground\n",
    "cnt = 0\n",
    "EPOCHS = 1000\n",
    "D=100\n",
    "ML=.8\n",
    "BATCH_SIZE=128\n",
    "y = y.astype(np.float32)\n",
    "for tr_idx, val_idx in kf.split(z):\n",
    "    cnt += 1\n",
    "    print(f\"FOLD {cnt}\")\n",
    "    net = make_model(nh,D,ML)\n",
    "    net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n",
    "    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n",
    "    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n",
    "    print(\"predict val...\")\n",
    "    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n",
    "    print(\"predict test...\")\n",
    "    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) / NFOLD\n",
    "    \n",
    "np.corrcoef(y, pred[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(y, pred[:, 1],\"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_opt = mean_absolute_error(y, pred[:, 1])\n",
    "unc = pred[:,2] - pred[:, 0]\n",
    "sigma_mean = np.mean(unc)\n",
    "print(sigma_opt, sigma_mean)\n",
    "idxs = np.random.randint(0, y.shape[0], 100)\n",
    "plt.plot(y[idxs], label=\"ground truth\")\n",
    "plt.plot(pred[idxs, 0], label=\"q25\")\n",
    "plt.plot(pred[idxs, 1], label=\"q50\")\n",
    "plt.plot(pred[idxs, 2], label=\"q75\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())\n",
    "plt.hist(unc)\n",
    "plt.title(\"uncertainty in prediction\")\n",
    "plt.show()\n",
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laplace_log_likelihood(y_true=tr['moistureOffPercent'], y_pred=tr['moistureOffPercent'], confidence=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['moistureOffPercent1'] = 1.*pe[:, 1]\n",
    "sub['Confidence1'] = pe[:, 2] - pe[:, 0]\n",
    "subm = sub[['ID_day','moistureOffPercent','Confidence','moistureOffPercent1','Confidence1']].copy()\n",
    "subm.loc[~subm.moistureOffPercent1.isnull()].head(10)\n",
    "subm.loc[~subm.moistureOffPercent1.isnull(),'moistureOffPercent'] = subm.loc[~subm.moistureOffPercent1.isnull(),'moistureOffPercent1']\n",
    "if sigma_mean<70:\n",
    "    subm['Confidence'] = sigma_opt\n",
    "else:\n",
    "    subm.loc[~subm.moistureOffPercent1.isnull(),'Confidence'] = subm.loc[~subm.moistureOffPercent1.isnull(),'Confidence1']\n",
    "subm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "subm.describe().T\n",
    "tr = pd.read_excel(open('../input/dryierbindatafornurserycorn/Drying formula.xlsx', 'rb'),\n",
    "              sheet_name='Sheet2') \n",
    "\n",
    "otest =  tr.loc[tr.Datetype == \"Test\"] \n",
    "#tr[\"ID\"] = range(1, 1 + len(tr))\n",
    "otest[\"a\"] = \"sub\"\n",
    "otest[\"b\"] = range(1, 1 + len(otest))\n",
    "otest[\"ID\"] = otest[\"a\"] + otest[\"b\"].map(str)\n",
    "\n",
    "for i in range(len(otest)):\n",
    "    subm.loc[subm['ID_day']==otest.ID[i]+'_'+str(otest.b[i]), 'moistureOffPercent'] = otest.moistureOffPercent[i]\n",
    "    subm.loc[subm['ID_day']==otest.ID[i]+'_'+str(otest.b[i]), 'Confidence'] = 0.1\n",
    "subm[[\"ID_day\",\"moistureOffPercent\",\"Confidence\"]].to_csv(\"submission_regression.csv\", index=False)\n",
    "reg_sub = subm[[\"ID_day\",\"moistureOffPercent\",\"Confidence\"]].copy()\n",
    "#df1 = img_sub.sort_values(by=['ID_day'], ascending=True).reset_index(drop=True)\n",
    "df2 = reg_sub.sort_values(by=['ID_day'], ascending=True).reset_index(drop=True)\n",
    "#df = df1[['ID_day']].copy()\n",
    "#df['FVC'] = FVC_weight*df1['moistureOffPercent'] + (1-FVC_weight)*df2['moistureOffPercent']\n",
    "#df['Confidence'] = Confidence_weight*df1['Confidence'] + (1-Confidence_weight)*df2['Confidence']\n",
    "df2.head();\n",
    "df2.to_csv('submission.csv', index=False)\n",
    "\n",
    "#df_issa3 = df.copy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
